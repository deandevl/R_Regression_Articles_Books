---
title: "Data Analysis Using Regression and Multilevel Models"
author: "Rick Dean"
format:
  html:
    toc: false
    toc-depth: 4
    toc-location: "right"
    number-sections: true
    number-offset: 2
    self-contained: true
    smooth-scroll: true
    code-fold: true
    code-block-bg: "#f1f3f5"
    code-block-border-left: "#31BAE9"
    code-overflow: wrap
    tbl-cap-location: "bottom"
    fig-width: 12
    fig-height: 6
    fig-align: "center"
    fig-cap-location: "bottom"
    minimal: false
    css: ../../style.css
    link-external-newwindow: true
    abstract-title: "Abstract"
    abstract: "The following R script are notes on Chapter 3 'Linear Regression: the basics' from [Data Analysis Using Regression and Multilevel-Hierarchical Models](http://www.stat.columbia.edu/~gelman/arm/) by Gelman, Hill"
---

::: topic
Load the Required R Packages:
:::

```{r}
#| warning: false
#| message: false

library(data.table)
library(purrr)
library(ggplot2)
library(foreign)
library(here)
library(magrittr)
library(RColorBrewer)
library(RplotterPkg)
library(RregressPkg)
```

# Linear Regression: the basics

## One predictor

::: topic
Regression with a binary predictor. Model children's test scores given a binary indicator for whether the mother graduated from high school.
:::

Read the data.

```{r}
read_kid_score_data <- function(){
  file_path <- file.path(here::here(), "books", "Data Analysis Using Regression", "data",  "kidiq.dta")
  
  kid_score_dt <- foreign::read.dta(file_path) %>%
    data.table::as.data.table() %>%
    .[, .(kid_score, mom_hs, mom_iq)] 
  return(kid_score_dt)
}
kid_score_dt <- read_kid_score_data()
```

Regress *kid_score* on *mom_hs*:
```{r}
#| tbl-cap: Binary regression - kid_score ~ mom_hs

binary_ols <- RregressPkg::ols_calc(
  df = kid_score_dt,
  formula_obj = kid_score ~ mom_hs
)

RplotterPkg::create_table(
  x = binary_ols$coef_df,
  container_width_px = 400
)
```

$$\hat{y}_{kid-score} = 78+12\cdot\beta_{mom-hs}+\epsilon$$

> This coefficient tells us that children of mothers who have completed high school score 12 points higher on average than children of mothers who have not completed high school.

```{r}
#| fig-cap: Child test score regressed on mothers completing/not completing high school

kid_score_dt[, binary_fitted_val := binary_ols$fitted_vals]
RplotterPkg::create_scatter_plot(
  df = kid_score_dt,
  aes_x = "mom_hs",
  aes_y = "kid_score",
  x_title = "Mother completed high school",
  y_title = "Child test score",
  x_major_breaks = c(0,1),
  position = position_jitter(width = 0.1, height = 0.00),
  y_major_breaks = seq(20, 140, 20),
  y_minor_breaks = NULL
) + geom_line(aes(y = binary_fitted_val), color="blue")
```

For a binary predictor, the regression coefficient is the difference between the averages of the two groups.

Compute the difference of average child test scores for mothers completing/not completing high school.

```{r}
mean_scores_without_hs <- mean(kid_score_dt[mom_hs == 0,]$kid_score)
mean_scores_with_hs <- mean(kid_score_dt[mom_hs == 1,]$kid_score)
mean_scores_diff <- mean_scores_with_hs - mean_scores_without_hs
```

The difference in mean scores is `r mean_scores_diff`. Compare with the above `mom_hs` coefficient from the OLS calculation. 

:::topic
Regression with a continuous predictor. Model child test score on mother's continuous IQ score.
:::

```{r}
#| tbl-cap: Continuous model - kid_score ~ mom_iq

continuous_ols <- RregressPkg::ols_calc(
  df = kid_score_dt,
  formula_obj = kid_score ~ mom_iq
)

RplotterPkg::create_table(
  x = continuous_ols$coef_df,
  container_width_px = 400
)
```

$$\hat{y}_{kid-score}=26+0.6\cdot\beta_{mom-iq}+\epsilon$$

> ...mothers' IQ differed by 10 points--these children would be expected to have scores that differed by 6 points on average.

> ...the intercept of 26 reflects the predicted test scores for children whose mothers have an IQ of zero. This is not the most helpful quantity.

```{r}
#| fig-cap: Continuous model - kid_score ~ mom_iq

kid_score_dt[, continuous_fitted_val := continuous_ols$fitted_vals]
RplotterPkg::create_scatter_plot(
  df = kid_score_dt,
  aes_x = "mom_iq",
  aes_y = "kid_score",
  x_title = "Mother IQ",
  y_title = "Child test score",
  y_major_breaks = seq(20, 140, 20),
  y_minor_breaks = NULL
) + geom_line(aes(y = continuous_fitted_val), color="blue")
```


## Multiple predictors

::: topic
Regress *kid_score* with *mom_hs* and *mom_iq* as predictors.
:::

```{r}
#| tbl-cap: Multiple predictors - kid_score ~ mom_hs + mom_iq

multi_ols <- RregressPkg::ols_calc(
  df = kid_score_dt,
  formula_obj = kid_score ~ mom_hs + mom_iq
)

RplotterPkg::create_table(
  x = multi_ols$coef_df,
  container_width_px = 400
)
```

$$\hat{y}_{kid-score} = 26 + 6\cdot\beta_{mom-hs} + 0.6\cdot\beta_{mom-iq} + \epsilon$$  

> This model forces the slope of the regression of child's test score on mother's IQ score to be the same for each maternal education subgroup.

* Interpret the coefficients
    + *Intercept* of 26: For a mom with zero high school and IQ, then the child test score is 26. This is not useful because no mothers have a zero IQ.
    + $\beta_{mom-hs}$: For mothers with the same IQ and high school, we would expect a 6 point higher child test score than those moms without high school.
    + $\beta_{mom-iq}$: For mothers with the same high school status and a 1 point higher IQ value, we would expect a .6 points higher difference in the child's test score.

```{r}
#| fig-cap: Multiple predictors - kid_score ~ mom_hs + mom_iq

kid_score_dt[, `:=`(
  mom_hs = as.factor(mom_hs), 
  multiple_fitted_val = multi_ols$fitted_vals
)]

RplotterPkg::create_scatter_plot(
  df = kid_score_dt,
  aes_x = "mom_iq",
  aes_y = "kid_score",
  aes_fill = "mom_hs",
  x_title = "Mother IQ score",
  y_title = "Child test score",
  pts_size = 2.0,
  y_major_breaks = seq(20, 140, 20),
  y_minor_breaks = NULL
) + 
geom_line(data = kid_score_dt[mom_hs == 0,], aes(y = multiple_fitted_val),     color = "red", alpha = 0.5, linewidth = 2.0) +
geom_line(data = kid_score_dt[mom_hs == 1,], aes(y = multiple_fitted_val),     color = "green", alpha = 0.5, linewidth = 2.0) +
ggplot2::scale_fill_manual(
  values = c("red","green")
)  
```

* Superimposed lines 
    + green -- child test score regressed on mother IQ for mothers **with** high school
    + red -- child test score regressed on mother IQ for mothers **without** high school

The author give examples where it's not always possible to change one predictor while holding all others constant.

## Interactions

> ...Figure 3.3 suggests that the line slopes differ substantially. A remedy for this is to include an *interaction* between `mom_hs` and `mom_iq`.

Add an interaction term to the above multiple model.
```{r}
#| tbl-cap: Coeffients for kid_score ~ mom_hs + mom_iq + mom_hs * mom_iq

multiple_inter_ols <- RregressPkg::ols_calc(
  df = kid_score_dt,
  formula_obj = kid_score ~ mom_hs + mom_iq + mom_hs * mom_iq
)

RplotterPkg::create_table(
  x = multiple_inter_ols$coef_df,
  container_width_px = 500
)
```

$$\hat{y}_{kid-score} = -11 + 51\beta_{mom-hs} + 0.96\beta_{mom-iq} - 0.5\beta_{mom-hs}\beta_{mom-iq} + \epsilon$$

```{r}
#| fig-cap: Child test score regressed on mother completing high school, mother IQ, and their interaction.

kid_score_dt[, multiple_inter_fitted_val := multiple_inter_ols$fitted_vals]
RplotterPkg::create_scatter_plot(
  df = kid_score_dt,
  aes_x = "mom_iq",
  aes_y = "kid_score",
  aes_fill = "mom_hs",
  x_title = "Mother IQ score",
  y_title = "Child test score",
  pts_size = 2.0,
  y_major_breaks = seq(20, 140, 20),
  y_minor_breaks = NULL
) + geom_line(data = kid_score_dt[mom_hs == 0], aes(y = multiple_inter_fitted_val), color = "red", alpha = 0.5, linewidth = 2.0) +
  geom_line(data = kid_score_dt[mom_hs == 1], aes(y = multiple_inter_fitted_val), color = "green", alpha = 0.5, linewidth = 2.0) +
ggplot2::scale_fill_manual(
  values = c("red","green")
)  
```


Understanding the model by looking at the separate regression lines for moms with and without high school.

* Interpret the coefficients
    + No high school
$$\hat{y}_{kid-score} = -11 + 51\cdot0+0.96\cdot\beta_{mom-iq}-0.5\cdot0\cdot\beta_{mom-iq}$$
$$= -11 + 0.96\cdot\beta_{mom-iq}$$

    + With high school
$$\hat{y}_{kid-score} = -11 +51\cdot1+0.96\cdot\beta_{mom-iq}-0.5\cdot1\cdot\beta_{mom-iq}$$
$$=40+0.46\cdot\beta_{mom-iq}$$

Thus the $-0.5\beta_{mom-hs}\beta_{mom-iq}$ in the model represents the difference in the slopes of moms with and without high school regression lines.

> When should we look for interactions. In practice, inputs that have large main effects also tend to have large interactions with other inputs(however small main effects do not preclude the possibility of large interactions).

Example is given of home radon and its interaction with the main effect smokers/nonsmokers in the likelihood of cancer.

## Statistical Inference

::: topic
Standard errors: uncertainty in the coefficient estimates.
:::

> We can roughly say that coefficient estimates within 2 standard errors of $\hat{\beta}$ are consistent.

```{r}
#| fig-cap: 95% region for mom_iq coefficient values

density_fun <- function(x){return(stats::dnorm(x, mean = 5.9, sd = 2.2))}
data_dt <- data.table(
  x_vals = seq(0, 15, 0.1)
) %>% 
.[, y_vals := unlist(purrr::map(x_vals, density_fun))]

RplotterPkg::create_scatter_plot(
  df = data_dt,
  aes_x = "x_vals",
  aes_y = "y_vals",
  x_title = "mom_iq coefficients",
  x_limits = c(0,16),
  x_major_breaks = seq(0,16,2),
  connect = T,
  show_pts = F
) + 
geom_segment(
  x = 1.5,
  y = 0,
  xend = 1.5,
  yend = 0.02454135,
  color = "red"
) +
annotate(
  geom = "text",
  label = "1.5 (-2 se)",
  x = 3,
  y = 0.02454135
)+ 
geom_segment(
  x = 10.3,
  y = 0,
  xend = 10.3,
  yend = .02454135,
  color = "red"
) + 
annotate(
  geom = "text",
  label = "10.3 (2 se)",
  x = 13,
  y = 0.02454135
) +
geom_segment(
  x = 5.9,
  y = 0,
  xend = 5.9,
  yend = 0.181337400,
  color = "blue"
) +
annotate(
  geom = "text",
  label = "mom_iq 5.9",
  x = 10,
  y = 0.16
) +
geom_ribbon(
  data = data_dt[x_vals > 1.5 & x_vals < 10.3],
  aes(x = x_vals, y = y_vals, ymin = 0, ymax = y_vals),
  fill = "gray",
  alpha = 0.3
)
```

Residuals, $r_{i}$. The difference between the data and the fitted values.

> If the model includes a constant term, then the residuals must be uncorrelated with a constant, which means they must have a mean of 0. This is a byproduct of how the model is estimated; it is *not* a regression assumption.

::: topic
Residual standard deviation $\hat{\sigma}$ and explained variance $R^{2}$.
:::

> ...we can think of (the) residual standard deviation as a measure of the average distance each observation falls from its prediction from the model.

$R^{2}$ is the fraction of data's variance ($s^{2}_{y}$) "explained" by the model. 

$$R^{2} = 1 - \hat{\sigma}^{2}/s^{2}_{y}$$
The quantity $n-k$, the number of data points minus the number of estimated coefficients is the *degrees of freedom*.

::: topic
Statistical significance.
:::

> Roughly speaking, if a coefficient is more than 2 standard errors away from zero, then it is called *statistically significant*.

> It is fine to have nonsignificant coefficients in a model, as long as they make sense.

::: topic
Uncertainty in the residual standard deviation.
:::

> Under the model, the estimated residual variance, $\hat{\sigma^{2}}$, has a sampling distribution centered at the the true $\sigma^{2}$ and proportional to a $\chi^{2}$ distribution with $n-k$ degrees of freedom.

## Graphical displays of data and fitted model

::: topic
Displaying uncertainty in the fitted regression.
:::

Using the $\hat{y}_{kid-score}$ regressed on $mom-iq$ we can run simulations of varying means of the predictor to show the uncertainty of the model.

```{r}
kid_score_dt <- read_kid_score_data()
continuous_ols <- RregressPkg::ols_calc(
  df = kid_score_dt,
  formula_obj = kid_score ~ mom_iq
)
kid_score_dt[, fitted := continuous_ols$fitted_vals]
```

::: topic
Simulate 10 predictions of the response variable from the kid_score ~ mom_iq model.
:::

```{r}
#| fig-cap: 10 Simulations of kid_score ~ mom_iq

sim_df <- stats::simulate(
  seed = 2345,
  object = lm(kid_score ~ mom_iq, data = kid_score_dt),
  nsim = 10
)
kid_score_sim_df <- cbind(kid_score_dt, sim_df)

aplot <- RplotterPkg::create_scatter_plot(
  df = kid_score_sim_df,
  aes_x = "mom_iq",
  aes_y = "kid_score",
  x_title = "Mother IQ score",
  y_title = "Child test score",
  pts_size = 2.0,
  y_major_breaks = seq(20, 140, 20),
  y_minor_breaks = NULL
) +
  geom_line(aes(y = fitted),color = "red", linewidth=2)

for(colname in colnames(sim_df)){
  dt <- data.table(
    mom_iq = kid_score_dt$mom_iq,
    sim_score = sim_df[[colname]]
  )
  sim_score_ols <- RregressPkg::ols_calc(df = dt, formula_obj = sim_score ~ mom_iq)
  dt[, fitted_vals := sim_score_ols$fitted_vals]
  aplot <- aplot +
    geom_line(data = dt, aes(x = mom_iq, y = fitted_vals), color = "blue", alpha = 0.8)
}
aplot
```

## Assumptions and diagnostics

:::topic
Plotting residuals to reveal aspects of the data not captured by the model
:::

```{r}
#| fig-cap: Residual plot for kid_score ~ mom_iq

kid_score_dt <- read_kid_score_data()
continuous_ols <- RregressPkg::ols_calc(
  df = kid_score_dt,
  formula_obj = kid_score ~ mom_iq
)
kid_score_dt[, residuals := continuous_ols$residual_vals]
residuals_sd <- sd(kid_score_dt$residuals)

RplotterPkg::create_scatter_plot(
  df = kid_score_dt,
  aes_x = "mom_iq",
  aes_y = "residuals",
  x_title = "Mother IQ score",
  y_title = "Residuals",
  pts_size = 2.0,
  y_major_breaks = seq(-60, 60, 20),
  y_minor_breaks = NULL
) +
  geom_hline(yintercept = 0, color = "red") +
  geom_hline(yintercept = residuals_sd, color = "blue", linetype = "dashed") +
  geom_hline(yintercept = -residuals_sd, color = "blue", linetype = "dashed")
```

## Prediction and validation

::: topic
Get a point `kid_score` prediction from `mom_iq` = 100 and `mom_hs` = 1.
:::

```{r}
#| tbl-cap: "Prediction of kid_score with mom_iq = 100 and mom_hs = 1"

kid_score_dt <- read_kid_score_data()

new_X <- data.frame(
  mom_iq = 100,
  mom_hs = 1
)
formula_obj <- lm(formula = kid_score ~ mom_hs + mom_iq, data = kid_score_dt)
predict_mom_100_1 <- stats::predict(object = formula_obj, newdata = new_X, interval = "prediction", level = 0.95)

RplotterPkg::create_table(
  x = data.frame(
    fit = predict_mom_100_1[[1]], 
    lower = predict_mom_100_1[[2]],
    upper = predict_mom_100_1[[3]]
  ),
  container_width_px = 400
)
```

::: topic
External validation
:::

Read the data and normalize mom iq to have mean of 100 and sd of 15. Data was collected from 1986 and 1994 for children born before 1987.
```{r}
file_path <- file.path(here::here(), "books", "Data Analysis Using Regression", "data",  "kid.iq.txt")
kid_iq_dt <- data.table::fread(file_path) %>% 
  .[, `:=`(
    afqt = (afqt.adj-mean(afqt.adj))*(15/sd(afqt.adj)) + 100,
    hs = as.numeric(educ.cat != 1)
  )] %>% 
  .[, .(ppvt, afqt, hs)]
```

Fit the model - ppvt ~ hs + afqt
```{r}
fit_all_ols <- RregressPkg::ols_calc(
  df = kid_iq_dt,
  formula_obj = ppvt ~ hs + afqt,
  print_detail = T
)
fit_coef <- fit_all_ols$coef_vals
```

Read the external data of children born in 1987 or later. 
```{r}
file_path <- file.path(here::here(), "books", "Data Analysis Using Regression", "data",  "kid.iq.ext.val.txt")
kid_iq_ev_dt <- data.table::fread(file_path) %>%
  .[, `:=`(
    afqt = (afqt.adj-mean(afqt.adj))*(15/sd(afqt.adj)) + 100,
    hs = as.numeric(educ.cat != 1)
  )] %>%
  .[, .(ppvt, afqt, hs)]
```

From the above model compute the predicted scores from the external data.
```{r}
predict_scores_v <- fit_coef[["(Intercept)"]] + fit_coef[["hs"]]*kid_iq_ev_dt$hs + fit_coef[["afqt"]]*kid_iq_ev_dt$afqt
```

Compute the residuals and its sd.
```{r}
residual_scores_v <- kid_iq_ev_dt$ppvt - predict_scores_v
residual_scores_sd <- sd(residual_scores_v)
```

Plot predicted scores versus actual scores
```{r}
plot_df <- data.frame(
  x = predict_scores_v,
  y = kid_iq_ev_dt$ppvt
)
RplotterPkg::create_scatter_plot(
  df = plot_df,
  aes_x = "x",
  aes_y = "y",
  x_title = "Predicted score",
  y_title = "Actual score",
  x_limits = c(20,140),
  y_limits = c(20,140),
  x_major_breaks = seq(20,140,40),
  y_major_breaks = seq(20,140,40)
) +
  geom_abline(slope = 1, color="red")
```

Plot predicted score versus residuals
```{r}
plot_df <- data.frame(
  x = predict_scores_v,
  y = residual_scores_v
)
RplotterPkg::create_scatter_plot(
  df = plot_df,
  aes_x = "x",
  aes_y = "y",
  x_title = "Predicted score",
  y_title = "Prediction error",
  y_limits = c(-80,50),
  y_major_breaks = seq(-80,60,20),
  x_minor_breaks = NULL,
  y_minor_breaks = NULL
) +
  geom_hline(yintercept = 0, color = "red") +
  geom_hline(yintercept = residual_scores_sd, color = "blue", linetype = "dashed") +
  geom_hline(yintercept = -residual_scores_sd, color = "blue", linetype = "dashed")
  
```

