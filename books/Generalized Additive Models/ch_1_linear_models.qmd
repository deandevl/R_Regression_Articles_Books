---
title: "Generalized Additive Models, An Introduction with R"
author: "Rick Dean"
format: 
  html:
    toc: false
    toc-depth: 4
    toc-location: "right"
    number-sections: false
    number-offset: 0
    self-contained: true
    smooth-scroll: true
    code-fold: true
    code-block-bg: "#f1f3f5"
    code-block-border-left: "#31BAE9"
    code-overflow: wrap
    tbl-cap-location: "bottom"
    fig-width: 12
    fig-height: 6
    fig-align: "center"
    fig-cap-location: "bottom"
    css: ../../style.css
    link-external-newwindow: true
    abstract-title: "Abstract"
    abstract: "Notes and graphics from the text [Generalized Additive Models, An Introduction with R, Second Edition](https://www.routledge.com/Generalized-Additive-Models-An-Introduction-with-R-Second-Edition/Wood/p/book/9781498728331) by Simon N. Wood. The following R script is following Chapter 1, 'Linear Models'"
---


::: topic
Load the Required R Packages:
:::

```{r}
#| warning: false
#| message: false

library(data.table)
library(purrr)
library(ggplot2)
library(here)
library(gt)
library(gamair)
library(magrittr)
library(RplotterPkg)
library(RregressPkg)
library(RmatrixPkg)
```

# Linear Models

## 1.3 The theory of linear models
### 1.3.1 Least squares estimation of $\beta$

We will be using the `gamair::sperm.comp1` data set and *RmatrixPkg::qr_householder()* for performing the QR decomposition. The data set is described in `[Wood, section 1.5.1 page 23]`. 

1. Load the data and define the predictor($\textbf{X}$) and response($\textbf{Y}$) matrices:
```{r}
data("sperm.comp1", package = "gamair")
sperm1_dt <- data.table::as.data.table(sperm.comp1) %>% 
  .[, .(count = count, time_ipc = time.ipc, prop_partner = prop.partner)]

n = nrow(sperm1_dt)

X_dt <- data.table(Intercept = rep(1, n)) %>% 
  .[, `:=` (time_pc = sperm1_dt$time_ipc, prop_partner = sperm1_dt$prop_partner)]

p = ncol(X_dt) # number of predictors including the intercept

X <- as.matrix(X_dt)                  # n x p
y <- as.matrix(sperm1_dt[, .(count)]) # n x 1
```

2. Perform $\textbf{QR}$ decomposition of $\textbf{X}$ into its components $\textbf{Q}_{f}$ and $\textbf{R}$ `[Wood Equation 1.5 page 12]`
```{r}
X_qr <- RmatrixPkg::qr_householder(X,complete = T)

Q <- X_qr$Q              # n x n
Qf <- Q[,1:p]            # n x p
R <- X_qr$R[1:p, 1:p]    # p x p
```
We currently have the following:

-   $n$ the number of observations
-   $p$ the number of predictors including the intercept
-   $\textbf{Q}$ an $n\space by \space n$ orthongonal matrix
-   $\textbf{Q}_{f}$ an $n\space by \space p$ matrix
-   $\textbf{R}$ a $p\space by \space p$ lower triangular matrix

:::topic
Check the decomposition where $\textbf{X} = \textbf{Q}_{f}\textbf{R}$.
:::
```{r}
Xcheck <- Qf %*% R
```

`r RmatrixPkg::matrix_to_latex(X[1:6,], bracket_type = "paren", matrix_name = "X")`


`r RmatrixPkg::matrix_to_latex(Xcheck[1:6,], bracket_type = "paren", matrix_name = "Xcheck")`

3. Compute $t(\textbf{Q})\textbf{y}$ and assign vectors $\textbf{f}$ and $\textbf{r}$:
```{r}
Qy <- t(Q) %*% y                  # n x 1

f <- as.matrix(Qy[1:p,])          # p x 1
r <- as.matrix(Qy[(p + 1):n,])    # n - p x 1
```

We have the following:

-   $f$ an $3\space by \space 1$ matrix
-   $r$ an $12\space by \space 1$ matrix

4. Estimate the  $\beta$'s `[Wood Equation 1.6 page 12]`:
```{r}
R_inv <- solve(R)               # p x p
beta_hat <- (R_inv %*% f)       # p x 1

dt <- data.table::as.data.table(beta_hat, keep.rownames = T) %>% 
  data.table::setnames(., old = colnames(.), new = c("Predictor","Beta"))
RplotterPkg::create_table(
  x = dt,
  decimals_lst = list(cols = c(2), decimal = 2),
  container_width_px = 300
)
```

:::takeaway
Compare the above betas with $\hat{\beta_{j}}$ at `[Wood, page 27]`
:::

### 1.3.2 The distribution of $\hat{\beta}$

The variance of the betas $var(\hat{\beta})$ are proportional to the unknown $\sigma^2$ variance of $\epsilon_{i}$ where $y_{i} = \beta x_{i} + \epsilon_{i}$ `[Wood Equation 1.3, page 4]`. An unbiased estimator of $\sigma^2$ is from the model's residuals $\hat{\epsilon_{i}} = y_{i} - x_{i}\hat{\beta}$ where:
$$\hat{\sigma}^2 = \frac{1}{N - 1}\sum(y_{i} - x_{i}\hat{\beta})$$
From `[Wood Equation 1.8, page 13]`:
$$\hat{\sigma}^2 = \frac{||r||^{2}}{(n - p)}$$

```{r}
sigma_sq <- drop(t(r) %*% r)/(n - p)
```
$\hat{\sigma^{2}}$ variance: 18661.91
  
Using $\hat{\sigma^{2}}$, Wood derives the covariance matrix of $\hat{\beta}$ `[Wood Equation 1.7, page 13]`:
$$V_{\hat{\beta}} = R^{-1}R^{-T}\hat{\sigma}^{2}$$
```{r}
Vbeta <- R_inv %*% t(R_inv) * sigma_sq # p x p
```

`r RmatrixPkg::matrix_to_latex(Vbeta, bracket_type = "paren", matrix_name = "Vbeta")`

Wood points out the distribution of $\hat{\beta}$:
$$\hat{\beta} \sim N(\beta,V_{\hat{\beta}}) $$

### 1.3.3 $(\hat{\beta_{i}} - \beta_{i})/\hat{\sigma}_{\hat{\beta_{i}}} \sim t_{n-p}$

Wood shows that the estimated standard error $\hat{\sigma_{\beta_{i}}}$ are the square root of the diagonal elements of $V_{\hat{\beta}}$.
```{r}
se_beta <- sqrt(diag(Vbeta))
```

The $\hat{\sigma_{\beta_{i}}}$ standard errors: `r se_beta`

Further Wood shows that $T_{i} = \hat{\beta_{i}}/\hat{\sigma_{\beta_{i}}}$ is a standardized measure of how far each beta parameter estimate is from zero. Under $H_{0}:\beta_{i} = 0$, $T_{i} \sim t_{n-p}$.

### 1.3.4 $F$-ratio results

Testing if $\beta_{time_ipc}$ and $\beta_{prop_partner}$  = 0 (i.e. **$\beta_{1}$** = 0)

$H_{0}:\beta_{1} = 0$ versus  $H_{1}: \beta_{1} \ne 0$

1. Partition **X** into **X_0** and **X_1**:
```{r}
p = 3
q = 2

X_0 <- X[,1]
X_1 <- X[,2:3]
```

2. Partition **f** and compute the increase in residual sum of squares that results from dropping **X_1**:

```{r}
f_0 <- f[1]
f_1 <- f[2:3]

sse_minus_beta_1 <- (t(f_1) %*% f_1)[1,1]
```

3. Compute $F$ which follows an $F$ distribution with *q* (2) and *n - p* (12) degrees of freedom:
```{r}
F_val <- (sse_minus_beta_1/q)/sigma_sq
```
$F$ = `r F_val`

4. Compute the $F$ critical value with 2 and 12 df at 1% level
```{r}
F_cv <- qf(1 - 0.01, 2,12)
```
$F_{q,n-p}$ = `r F_cv`

:::takeaway
The $F$ is less than the critical value, so we do not reject the hypothesis that *time_ipc* and *prop_partner* are zero. We need more predictors than just the intercept alone in our model. 
:::

## 1.5 Practical linear modeling

### 1.5.1 Model fitting and model checking

:::topic
Load and plot the raw data.
:::
```{r}
#| fig-cap: "Pair plot of sperm competition data"
#| fig-height: 8

data("sperm.comp1", package = "gamair")
sperm1_dt <- data.table::as.data.table(sperm.comp1) %>% 
  .[, .(subject = subject, count = count, time_ipc = time.ipc, prop_partner = prop.partner)]

RregressPkg::plot_pairs(
  df = sperm1_dt,
  var_name_scaling = list(
    time_ipc = NULL,
    prop_partner = NULL,
    count = NULL
  ),
  axis_text_sz = 14,
  plot_dim = 20
)
```

:::takeaway
There appears to some decrease in sperm count (*count*) as proportion of time spent together (*prop_partner*) increases.
:::

:::topic
Define the initial linear model.
:::
```{r}
sperm1_ols_lst <- RregressPkg::ols_calc(
  df = sperm1_dt,
  formula_obj = count ~ time_ipc + prop_partner
)

RplotterPkg::create_table(
  x = sperm1_ols_lst$coef_df,
  container_width_px = 400
)
```

:::topic
Check the model.
:::
```{r}
#| fig-cap: "Model checking plots for the linear model: count ~ time_ipc + prop_partner"
#| fig-height: 8

RregressPkg::plot_model_check(
  fitted_v = sperm1_ols_lst$fitted_vals,
  response_v = sperm1_dt$count,
  residual_v = sperm1_ols_lst$residual_vals,
  id_v = sperm1_dt$subject,
  histo_fill = "blue",
  histo_alpha = 0.7
)
```



