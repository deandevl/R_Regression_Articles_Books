---
title: "Applied Predictive Modeling--Chapter 3 Data Pre-processing"
output: 
   html_document:
    toc: yes
    toc_depth: 3
    css: ../../style.css
params:
  date: !r Sys.Date()
---

```{r,setup, include=FALSE, eval=TRUE}
options(knitr.table.format = "html")
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, fig.width = 12, fig.height = 8)
```

<div>Author: Rick Dean</div>
<div>Article date: `r params$date`</div>

<div class="abstract">
  <p class="abstract">Abstract</p>
The following notes/scripts/plots are inspired by Chapter 3 Data Pre-processing of [Applied Predictive Modeling](http://appliedpredictivemodeling.com/) by Max Kuhn and Kjell Johnson.
</div>

<div class="note">Note: Look in the "chapters" directory of the AppliedPredictiveModeling package for the R scripts used in the book.</div>

```{r, warning = FALSE, message = FALSE}
library(data.table)
library(ggplot2)
library(ggrepel)
library(caret)
library(AppliedPredictiveModeling)
library(e1071)
library(grid)
library(gtable)
library(RregressPkg)
library(RplotterPkg)
```

## 3.1 Case Study: Cell Segmentation in High-Content Screening
<div class="task">Task: Prepare the training set. </div>

1. Access the data and retain the original training set:
```{r}
data(segmentationOriginal, package = "AppliedPredictiveModeling")
seg_orig_dt <- data.table::setDT(segmentationOriginal)
seg_data_dt <- seg_orig_dt[Case == "Train"]
```

2. Save the *Case*, *Class*, and *Cell* predictors in separate vectors:
```{r}
case_dt <- seg_data_dt[, .(Case)]
class_dt <- seg_data_dt[, .(Class)]
cell_dt <- seg_data_dt[, .(Cell)]
```

3. Remove the above 3 predictors from *seg_data_dt*:
```{r}
seg_data_dt <- seg_data_dt[, !c("Case", "Class", "Cell")]
```

4. Remove predictors that have "status" in their names:
```{r}
status_col_names <- base::grep("Status", colnames(seg_data_dt), value = T)
seg_data_dt <- seg_data_dt[, !..status_col_names]
```

## 3.2 Data Transformations for Individual Predictors
<div class="note">The column "VarIntenCh3" measures the standard deviation of the intensity
of the pixels in the actin filaments</div>

<div class="task">Task: Review the skewness of "VarIntenCh3". </div>

<blockquote>A general rule of thumb to consider is that skewed data whose ratio of the highest value to the lowest value is greater than 20 have significant skewness. </blockquote>

1. VarIntenCh3 ratio max/min:
```{r}
max(seg_data_dt$VarIntenCh3)/min(seg_data_dt$VarIntenCh3)
```
2. VarIntenCh3 skewness:
```{r}
e1071::skewness(seg_data_dt$VarIntenCh3)
```
<div class="task">Task: Use `caret::preProcess()` to transform for skewness.</div>

1. Run `caret::preProcess()` using "BoxCox" for *method*: 
```{r}
pre_process_lst <- caret::preProcess(seg_data_dt, method = "BoxCox")
```
The class for *pre_process_lst* is "preProcess" which tells `stats::predict()` to return a transformed data frame.

2. Apply *pre_process_lst* to transform *seg_data_dt* using `stats::predict()`:
```{r}
seg_data_dt <- data.table::setDT(stats::predict(pre_process_lst, seg_data_dt))
```

3. The $\lambda$ used in the transformation: 
```{r}
pre_process_lst$bc$VarIntenCh3$lambda
```

4. Plot the histogram of both the original and transformed data of predictor variable *VarIntenCh3*:
```{r, fig.width = 12, fig.height = 8}
plot_1 <- RplotterPkg::create_histogram_plot(
  df = seg_orig_dt[, .(VarIntenCh3)],
  aes_x = "VarIntenCh3",
  rot_y_tic_label = TRUE,
  bins = 10,
  bar_fill = "blue",
  x_title = "Natural Units"
)

plot_2 <- RplotterPkg::create_histogram_plot(
  df = seg_data_dt,
  aes_x = "VarIntenCh3",
  rot_y_tic_label = TRUE,
  bins = 10,
  bar_fill = "blue",
  x_title = "Log Units",
  do_y_title = FALSE
)

layout <- list(
  plots = list(plot_1, plot_2),
  rows = c(1,1),
  cols = c(1,2)
)

plot_3 <- RplotterPkg::multi_panel_grid(
  layout = layout,
  col_widths = c(5,5),
  row_heights = 7,
  title = "Box-Cox Transformation of VarIntenCh3",
  subtitle = "Cell Segmentation in High-Content Screening",
)
```

5. VarIntenCh3 skewness after Box-Cox transformation:
```{r}
e1071::skewness(seg_data_dt$VarIntenCh3)
```
## 3.3 Data Transformations for Multiple Predictors

<div class="task">Task: Use Principal Components Analysis (PCA) to find linear combinations of predictors.</div>

<blockquote>PCA seeks to find linear combinations of predictors, known as principal components (PC's) which capture the most possible variance.  The first PC is defined as the linear combination of the predictors that captures the most variability of all the possible linear combinations.</blockquote>

1. Consider two highly correlated predictors from *seg_data_dt* where PCA could be used to select one that explains the most variance:

   a. average pixel intensity of channel 1 (AvgIntenCh1)
   b. entropy of intensity of channel 1(EntropyIntenCh1)
    
2. Estimate the linear model between the two predictors:
```{r}
fit_lm <- stats::lm(AvgIntenCh1 ~ EntropyIntenCh1, data = seg_data_dt)
```

3. The correlation between the two predictors:
```{r}
summary(fit_lm)$r.squared
```

4. Show a scatter plot of the predictors with the *Class* predictor as an aes fill:
```{r}
df <- cbind(seg_data_dt, class_dt)
RplotterPkg::create_scatter_plot(
  df = df,
  aes_x = "EntropyIntenCh1",
  aes_y = "AvgIntenCh1",
  aes_fill = "Class",
  title = "EntropyIntenCh1 vs AvgIntenCh1",
  subtitle = "Cell Segmentation in High-Content Screening",
  x_title = "Channel 1 Intensity Entropy",
  y_title = "Channel 1 Fiber Width",
  rot_y_tic_label = TRUE,
  pts_line_alpha = 0.5,
  palette_colors = c("red","blue")
)
```

5. Compute the PCA involving the two predictors:
```{r}
pca <- stats::prcomp(formula = ~ AvgIntenCh1 + EntropyIntenCh1, data = seg_data_dt, scale. = TRUE)
```

6. Show a scatter plot to the two principal components:
```{r}
dt <- data.table::as.data.table(pca$x)
dt[, Class := class_dt$Class]

RplotterPkg::create_scatter_plot(
  df = dt,
  aes_x = "PC1",
  aes_y = "PC2",
  aes_fill = "Class",
  title = "Principal Components Involving AvgIntenCh1 + EntropyIntenCh1",
  subtitle = "Cell Segmentation in High-Content Screening",
  x_title = "Principal Component #1",
  y_title = "Principal Component #2",
  rot_y_tic_label = TRUE,
  pts_line_alpha = 0.5,
  x_limits = c(-4,4),
  x_major_breaks = seq(-4,4,1),
  y_limits = c(-4,4),
  y_major_breaks = seq(-4,4,1),
  palette_colors = c("red","blue")
)
```

<blockquote>The first PC summarizes 97% of the original variability, while the second summarizes 3%.  Hence it is reasonable to use only the first PC for modeling since it accounts for the majority of the information in the data.  </blockquote>

<div class="task">Task: Plot the pca using `RregressPkg::plot_pca()`</div>  
```{r,fig.width=13}
df <- cbind(seg_data_dt, class_dt)
pca_plot <- RregressPkg::plot_pca(
  df = df,
  center = TRUE,
  scale. = TRUE,
  measures = c("AvgIntenCh1", "EntropyIntenCh1"),
  aes_fill = "Class",
  x_limits = c(-4,4),
  x_major_breaks = seq(-4,4,1),
  y_limits = c(-4,4),
  y_major_breaks = seq(-4,4,1),
  figure_width = 12
)
```

<div class="task">Task: Apply principal components to the entire set of segmentation predictors.</div>

1. Remove those predictors that have only a single value:
```{r}
length_fun <- function(xx = NULL){
  if(length(unique(xx)) != 1){
    return(xx)
  }else{
    return(NULL)
  }
} 
seg_data_dt[,colnames(seg_data_dt) := lapply(seg_data_dt, length_fun)] 
```

2. Create scatterplot objects of PC1 vs PC2, PC1 vs PC3 and PC2 vs PC3 using RregressPkg::plot_pca():
```{r}
df <- cbind(seg_data_dt, class_dt)
measures <- colnames(seg_data_dt)

plot_1 <- RregressPkg::plot_pca(
  df = df,
  center = TRUE,
  scale. = TRUE,
  measures = measures,
  pca_pair = c("PC1", "PC2"),
  aes_fill = "Class",
  palette_colors = c("red", "blue"),
  figure_width = 4,
  display_plot = FALSE
)

plot_2 <- RregressPkg::plot_pca(
  df = df,
  center = TRUE,
  scale. = TRUE,
  measures = measures,
  pca_pair = c("PC1", "PC3"),
  aes_fill = "Class",
  palette_colors = c("red", "blue"),
  figure_width = 4,
  display_plot = FALSE
)

plot_3 <- RregressPkg::plot_pca(
  df = df,
  center = TRUE,
  scale. = TRUE,
  measures = measures,
  pca_pair = c("PC2", "PC3"),
  aes_fill = "Class",
  palette_colors = c("red", "blue"),
  figure_width = 4,
  display_plot = FALSE
)
```

3. Set a layout for the 3 plots and display:
```{r,fig.width=14, fig.height=7}
layout <- list(
  plots = list(plot_1$samp_plot, plot_2$samp_plot, plot_3$samp_plot),
  rows = c(1, 1, 1),
  cols = c(1, 2, 3)
)
col_widths <- c(4, 4, 4)
row_heights <- c(5)

pca_figure <- RplotterPkg::multi_panel_grid(
  layout = layout,
  col_widths = col_widths,
  row_heights = row_heights,
  title = "Cell Segmentation Data",
  subtitle = "First Three Principal Components",
  do_legend = TRUE
)
```

## 3.4 Dealing with Missing Values
(no scripts)

## 3.5 Removing Predictors
<div class="task">Task: Use the algorithm outlined on page 47 to identify segmentation predictors with low colinearity.</div>

1. Compute the correlation matrix for the *seg_data_dt*:
```{r}
seg_cor <- stats::cor(seg_data_dt)
```

2. Call `RregressPkg::low_predictor_collinearity()`:
```{r}
results_lst <- RregressPkg::low_predictor_collinearity(seg_cor)
```

The algorithm suggest of the 58 variables, 34 high correlations (to possible remove) and 24 low predictor correlations:
```{r}
results_lst$predictors
```


<div class="task">Task: Use `caret::findCorrelation()` to identify columns to remove.</div>  

```{r}
high_cor_indexes <- caret::findCorrelation(seg_cor, 0.75)
high_cor_names <- colnames(seg_cor)[high_cor_indexes]
low_cor_names <- colnames(seg_cor)[-high_cor_indexes]
```
Results for `caret::findCorrelation()`--

Number with high correlations: `r length(high_cor_names)`

Number with low correlations: `r length(low_cor_names)`

## 3.8 Creating Dummy Variables
<div class="task">Task: Create dummy variables from the *cars* data set in the caret package.</div>

1. Show the data set:
```{r}
data("cars", package = "caret")
cars_dt <- data.table::setDT(cars)
```

2. Define a "Type" factor:
```{r}
cars_dt[, Type := data.table::fcase(convertible == 1,"convertible",
                        coupe == 1,"coupe",
                        hatchback == 1,"hatchback",
                        sedan == 1,"sedan",
                        wagon == 1,"wagon")]
cars_dt[, Type := factor(Type)]
cars_dt <- cars_dt[,.(Price, Mileage, Type)]
head(cars_dt)
```

3. Define a subset of *cars_dt*:
```{r}
cars_subset_dt <- cars_dt[sample(1:nrow(cars_dt), 40),]
```

4. Use `caret::dummyVars()` to model Price as a simple additive function of *Mileage* and *Type*:
```{r}
simple_mod <- caret::dummyVars(Price ~ Mileage + Type, data = cars_subset_dt, levelsOnly = TRUE)
simple_mod
```
5. Use `stats::predict()` with the model to generate the dummy variables on *cars_dt*:
```{r}
cars_with_dummy_mt <- stats::predict(simple_mod, cars_dt)
head(cars_with_dummy_mt)
```
<blockquote>The *Type* was expanded into five variables for five factor levels. The model is simple because it assumes that the effect of the mileage is the same for every type of car. To fit a more advanced model, we could assume that there is a **joint** effect of mileage and car type. This type of effect is referred to as an interaction. In the model formula , a colon between factors indicates that an interaction should be generated.</blockquote>

<div class="task">Task: Create an interaction effects model.</div>

1. Define the interaction effects model:
```{r}
interact_mod <- caret::dummyVars(~ Mileage + Type + Mileage:Type, data = cars_subset_dt, levelsOnly = TRUE)
interact_mod
```
2. Generate the dummy variables using `stats::predict()` on *cars_dt*:
```{r}
cars_with_dummy_inter_mt <- stats::predict(interact_mod, cars_dt)
head(cars_with_dummy_inter_mt)
```

